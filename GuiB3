#pip install torch torchvision torchaudio
#pip install transformers
#pip install accelerate
#pip install bitsandbytes
#pip install gradio
#pip install langdetect


import os
import random
import asyncio
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed
import gradio as gr
import torch
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException

# Tu token de Hugging Face (reemplaza "tu_token_aquí" con tu token real)
HF_TOKEN = os.getenv("HF_TOKEN", "tu_token_aquí")

# Configuración global para el modelo y tokenizador
model = None
tokenizer = None

# Configuración personalizada por defecto
default_user_name = "Usuario"
default_chatbot_name = "Asistente"
default_response_word_limit = 150  # Límite de palabras
default_story_context = ""
default_seed = None  # Semilla por defecto
default_personality = "Amigable"

# Perfiles de Personalidad Predefinidos
personality_profiles = {
    'Amigable': "eres amigable y utilizas un lenguaje coloquial.",
    'Profesional': "eres muy formal y profesional en tus respuestas.",
    'Humorístico': "siempre respondes con humor apropiado.",
    'Empático': "eres muy empático y muestras comprensión en tus respuestas.",
    'Creativo': "ofreces respuestas creativas y originales.",
    # Puedes agregar más perfiles según desees
}

def load_model():
    """
    Carga el modelo LLaMA 3.2-3B con optimización de memoria.
    """
    global model, tokenizer
    llama_model_name = "meta-llama/Llama-3.2-3B"
    print("Cargando el tokenizador...")

    # Cargar el tokenizador con autenticación
    tokenizer = AutoTokenizer.from_pretrained(
        llama_model_name,
        use_fast=False,
        token=HF_TOKEN
    )

    print("Cargando el modelo...")
    # Configuración de BitsAndBytes para optimización en 8 bits
    bnb_config = BitsAndBytesConfig(
        load_in_8bit=True
    )

    # Cargar el modelo con autenticación
    model = AutoModelForCausalLM.from_pretrained(
        llama_model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
        token=HF_TOKEN
    )

    print("Modelo cargado exitosamente.")

def set_random_seed(seed):
    """
    Establece la semilla para los generadores de números aleatorios.
    """
    if seed is not None:
        set_seed(seed)
        random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

def detect_language(text):
    """
    Detecta el idioma del texto proporcionado.
    """
    try:
        return detect(text)
    except LangDetectException:
        return 'es'  # Asume español por defecto

def infer_gender(name):
    """
    Infiera el género basado en el nombre (simplificado).
    """
    if name.endswith('a') or name.endswith('A'):
        return 'ella'
    else:
        return 'él'

def generate_text(user_input, state):
    """
    Genera texto basado en el historial de conversación y la entrada del usuario.
    """
    if model is None or tokenizer is None:
        raise ValueError("El modelo no está cargado. Llama a load_model() primero.")

    # Establecer la semilla si se proporciona
    seed = state.get('seed', default_seed)
    set_random_seed(seed)

    # Desempaquetar variables de estado
    input_ids_history = state.get('input_ids_history', None)
    story_context = state['story_context']
    response_word_limit = state['response_word_limit']
    user_name = state['user_name'] or default_user_name
    chatbot_name = state['chatbot_name'] or default_chatbot_name
    generation_params = state['generation_params']
    personality = state.get('personality', default_personality)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Inferir el género del chatbot
    chatbot_gender = infer_gender(chatbot_name)

    # Prompt inicial mejorado con instrucciones detalladas
    if input_ids_history is None:
        # Prompt inicial con instrucciones estrictas
        prompt = f"""
El siguiente es un diálogo entre {user_name} y {chatbot_name}.

Contexto de la conversación:
{story_context}

Instrucciones para {chatbot_name}:
- {chatbot_name} {personality_profiles.get(personality, '')}
- Siempre responde de manera coherente y relevante a las preguntas y comentarios de {user_name}.
- No proporcionas información que no sea solicitada directamente.
- Mantienes el contexto de la conversación y te refieres a eventos anteriores cuando sea apropiado.
- Evitas desviarte del tema principal y no incluyes información irrelevante.
- Si no sabes la respuesta a una pregunta, lo admites educadamente y ofreces ayuda adicional si es posible.

La conversación comienza ahora:

{user_name}: {user_input}
{chatbot_name}:"""

        # Tokenizar el prompt inicial
        input_ids_history = tokenizer.encode(prompt.strip(), return_tensors='pt').to(device)
    else:
        # Añadir el mensaje del usuario al historial
        new_message = f"\n{user_name}: {user_input}\n{chatbot_name}:"
        new_user_input_ids = tokenizer.encode(new_message, return_tensors='pt').to(device)
        input_ids_history = torch.cat([input_ids_history, new_user_input_ids], dim=-1)

    # Verificar si el input_ids_history excede la longitud máxima
    max_length = 1024  # Ajustado para mejorar la velocidad
    if input_ids_history.size(1) > max_length:
        # Recortar el historial manteniendo los últimos tokens
        input_ids_history = input_ids_history[:, -max_length:]

    # Generar la respuesta del modelo con torch.inference_mode para optimizar
    with torch.inference_mode():
        outputs = model.generate(
            input_ids_history,
            max_new_tokens=150,  # Limitar para acelerar la respuesta
            temperature=generation_params['temperature'],
            top_p=generation_params['top_p'],
            repetition_penalty=generation_params['repetition_penalty'],
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.encode(f"\n{user_name}:", add_special_tokens=False)[0],
        )

    # Extraer la respuesta generada
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extraer solo la última respuesta del chatbot
    response = generated_text.split(f"{chatbot_name}:")[-1].split(f"{user_name}:")[0].strip()

    # Limitar la respuesta a la cantidad máxima de palabras
    if response_word_limit > 0:
        response = " ".join(response.split()[:response_word_limit])

    # Actualizar el estado
    state['input_ids_history'] = outputs

    # Actualizar el historial de mensajes para mostrar en la interfaz
    state['history'].append((f"{user_name}: {user_input}", f"{chatbot_name}: {response}"))

    return state

async def chat_interface(user_input, new_user_name, new_chatbot_name, new_story_context, new_word_limit, temperature, top_p, repetition_penalty, seed_input, personality_choice, state, reset_conversation):
    """
    Interfaz para interactuar con el modelo desde Gradio.
    """
    # Inicializar el estado si es None o si se ha reiniciado la conversación
    if state is None or reset_conversation:
        state = {
            'history': [],
            'input_ids_history': None,
            'user_name': new_user_name or default_user_name,
            'chatbot_name': new_chatbot_name or default_chatbot_name,
            'story_context': new_story_context or default_story_context,
            'response_word_limit': int(new_word_limit) if new_word_limit else default_response_word_limit,
            'seed': int(seed_input) if seed_input else default_seed,
            'personality': personality_choice or default_personality,
            'generation_params': {
                'temperature': temperature,
                'top_p': top_p,
                'repetition_penalty': repetition_penalty
            }
        }
        # No añadir mensaje inicial
        return [], state

    if not user_input.strip():
        # Si no hay entrada del usuario, devolver el historial actual
        return state['history'], state

    # Generar respuesta de forma asíncrona
    loop = asyncio.get_event_loop()
    state = await loop.run_in_executor(None, generate_text, user_input, state)

    # Preparar el historial para mostrar en la interfaz
    displayed_history = state['history']

    return displayed_history, state

def main():
    """
    Configura el modelo y lanza la interfaz interactiva con Gradio.
    """
    print("Cargando el modelo...")
    load_model()

    print("Iniciando la interfaz...")
    with gr.Blocks() as interface:
        state = gr.State()
        gr.Markdown("## Chatbot Mejorado by ViajaTech")

        # Historial de conversación en la parte superior
        chatbot_display = gr.Chatbot(
            label="Historial de la Conversación",
            height=500,
            show_label=True
        )

        # Sección de entrada y configuraciones debajo del chat
        with gr.Column():
            with gr.Row():
                user_input = gr.Textbox(label="Tu mensaje", placeholder="Escribe aquí tu mensaje", lines=2)
                submit_btn = gr.Button("Enviar")
                reset_btn = gr.Button("Reiniciar Conversación")
            with gr.Row():
                new_user_name = gr.Textbox(label="Tu nombre (opcional)", placeholder="Tu nombre")
                new_chatbot_name = gr.Textbox(label="Nombre del Chatbot (opcional)", placeholder="Nombre del chatbot")
            new_story_context = gr.Textbox(label="Contexto o Historia Inicial (opcional)", placeholder="Escribe aquí un contexto o historia", lines=3)
            new_word_limit = gr.Slider(label="Palabras Máximas en Respuesta", minimum=50, maximum=300, step=10, value=150)
            temperature = gr.Slider(label="Temperature", minimum=0.1, maximum=1.0, step=0.1, value=0.5)
            top_p = gr.Slider(label="Top-p", minimum=0.1, maximum=1.0, step=0.05, value=0.9)
            repetition_penalty = gr.Slider(label="Penalización por Repetición", minimum=1.0, maximum=2.0, step=0.05, value=1.2)
            seed_input = gr.Textbox(label="Semilla para Aleatoriedad (opcional)", placeholder="Ingresa un número entero")
            personality_choice = gr.Dropdown(label="Perfil de Personalidad", choices=list(personality_profiles.keys()), value=default_personality)

        inputs = [
            user_input,
            new_user_name,
            new_chatbot_name,
            new_story_context,
            new_word_limit,
            temperature,
            top_p,
            repetition_penalty,
            seed_input,
            personality_choice,
            state,
            gr.State(False)  # Estado para el botón de reinicio
        ]

        outputs = [
            chatbot_display,
            state
        ]

        # Función que maneja el evento de envío
        def handle_submit(*args):
            return asyncio.run(chat_interface(*args))

        # Conectar el botón de enviar
        submit_btn.click(
            handle_submit,
            inputs=inputs,
            outputs=outputs
        )

        # Función que maneja el evento de reinicio
        def handle_reset():
            return [], None

        # Conectar el botón de reinicio
        reset_btn.click(
            handle_reset,
            inputs=[],
            outputs=[chatbot_display, state]
        )

    interface.launch(share=False)

if __name__ == "__main__":
    main()
