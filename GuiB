#pip install --upgrade gradio
#pip install torch torchvision torchaudio transformers accelerate bitsandbytes gradio asyncio langdetect pillow
#pip install torch torchvision torchaudio
#pip install transformers
#pip install accelerate
#pip install bitsandbytes
#pip install gradio
#pip install asyncio
#pip install langdetect
#pip install pillow


import asyncio
import os
import random
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed
import gradio as gr
import torch
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException
from PIL import Image, ImageOps, ImageDraw
import base64
from io import BytesIO

# Tu token de Hugging Face (asegúrate de reemplazar "tu_token_aquí" con tu token real)
HF_TOKEN = os.getenv("HF_TOKEN", "tu_token_aquí")

# Configuración global para el modelo y tokenizador
model = None
tokenizer = None

# Configuración personalizada por defecto
default_user_name = "David"
default_chatbot_name = "Isabella"
default_response_word_limit = 50
default_story_context = ""
default_seed = None  # Semilla por defecto (None para aleatoriedad completa)
default_personality = "Amigable"

# Perfiles de Personalidad Predefinidos
personality_profiles = {
    'Amigable': "Es amigable y utiliza un lenguaje coloquial.",
    'Profesional': "Es muy formal y profesional en sus respuestas.",
    'Humorístico': "Siempre responde con humor apropiado.",
    'Empático': "Es muy empática y muestra comprensión en sus respuestas.",
    'Creativo': "Ofrece respuestas creativas y originales.",
    # Puedes agregar más perfiles según desees
}

def load_model():
    """
    Carga el modelo LLaMA 3.2-3B con optimización de memoria.
    """
    global model, tokenizer
    llama_model_name = "meta-llama/Llama-3.2-3B"
    print("Cargando el tokenizador...")

    # Cargar el tokenizador con autenticación
    tokenizer = AutoTokenizer.from_pretrained(
        llama_model_name,
        use_fast=False,
        token=HF_TOKEN
    )

    print("Cargando el modelo...")
    # Configuración de BitsAndBytes para optimización en 8-bit
    bnb_config = BitsAndBytesConfig(
        load_in_8bit=True
    )

    # Cargar el modelo con autenticación
    model = AutoModelForCausalLM.from_pretrained(
        llama_model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
        token=HF_TOKEN
    )

    print("Modelo cargado exitosamente.")

def set_random_seed(seed):
    """
    Establece la semilla para los generadores de números aleatorios.
    """
    if seed is not None:
        set_seed(seed)
        random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

def detect_language(text):
    """
    Detecta el idioma del texto proporcionado.
    """
    try:
        return detect(text)
    except LangDetectException:
        return 'es'  # Asume español por defecto

def infer_gender(name):
    """
    Infiera el género basado en el nombre (simplificado).
    """
    if name.endswith('a') or name.endswith('A'):
        return 'ella'
    else:
        return 'él'

def process_image(img):
    """
    Ajusta el tamaño de la imagen, la convierte en circular y la codifica en base64.
    """
    # Redimensionar la imagen
    img = img.resize((64, 64))

    # Crear una máscara circular
    mask = Image.new('L', img.size, 0)
    draw = ImageDraw.Draw(mask)
    draw.ellipse((0, 0) + img.size, fill=255)

    # Aplicar la máscara circular
    result = Image.new('RGBA', img.size)
    result.paste(img, (0, 0), mask=mask)

    # Convertir la imagen a base64
    buffered = BytesIO()
    result.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    return f"data:image/png;base64,{img_str}"

def generate_text(user_input, state):
    """
    Genera texto basado en el historial de conversación y la entrada del usuario.
    """
    if model is None or tokenizer is None:
        raise ValueError("El modelo no está cargado. Llama a load_model() primero.")

    # Establecer la semilla si se proporciona
    seed = state.get('seed', default_seed)
    set_random_seed(seed)

    # Desempaquetar variables de estado
    input_ids_history = state.get('input_ids_history', None)
    story_context = state['story_context']
    response_word_limit = state['response_word_limit']
    user_name = state['user_name'] or default_user_name
    chatbot_name = state['chatbot_name'] or default_chatbot_name
    generation_params = state['generation_params']
    personality = state.get('personality', default_personality)
    user_language = state.get('user_language', 'es')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Detectar el idioma del usuario
    user_language = detect_language(user_input)
    state['user_language'] = user_language

    # Inferir el género del chatbot
    chatbot_gender = infer_gender(chatbot_name)

    # Construir el prompt inicial si es la primera interacción
    if input_ids_history is None:
        # Prompt inicial con etiquetas claras
        prompt = f"""El siguiente es un diálogo entre {user_name} y su asistente virtual {chatbot_name}.

Contexto:
{chatbot_name} es {chatbot_gender} asistente virtual que {personality_profiles.get(personality, '')}
{story_context}

Conversación:
{user_name}: {user_input}
{chatbot_name}:"""

        # Tokenizar el prompt inicial
        input_ids_history = tokenizer.encode(prompt, return_tensors='pt').to(device)
    else:
        # Añadir el mensaje del usuario al historial
        new_message = f"\n{user_name}: {user_input}\n{chatbot_name}:"
        new_user_input_ids = tokenizer.encode(new_message, return_tensors='pt').to(device)
        input_ids_history = torch.cat([input_ids_history, new_user_input_ids], dim=-1)

    # Verificar si el input_ids_history excede la longitud máxima
    max_length = 2048
    if input_ids_history.size(1) > max_length:
        # Recortar el historial eliminando los tokens más antiguos
        input_ids_history = input_ids_history[:, -max_length:]

    # Generar la respuesta del modelo
    outputs = model.generate(
        input_ids_history,
        max_new_tokens=200,
        temperature=generation_params['temperature'],
        top_p=generation_params['top_p'],
        repetition_penalty=generation_params['repetition_penalty'],
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        eos_token_id=tokenizer.encode(f"\n{user_name}:", add_special_tokens=False)[0],
    )

    # Extraer la respuesta generada
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Extraer solo la última respuesta del chatbot
    response = generated_text.split(f"{chatbot_name}:")[-1].split(f"{user_name}:")[0].strip()

    # Limitar la respuesta a la cantidad máxima de palabras
    if response_word_limit > 0:
        response = " ".join(response.split()[:response_word_limit])

    # Actualizar el estado
    state['input_ids_history'] = outputs

    # Actualizar el historial de mensajes para mostrar en la interfaz
    state['history'].append(('assistant', response))

    return state

async def chat_interface(user_input, new_user_name, new_chatbot_name, new_story_context, new_word_limit, reset, temperature, top_p, repetition_penalty, seed_input, personality_choice, user_photo, chatbot_photo, state):
    """
    Interfaz para interactuar con el modelo desde Gradio.
    """
    # Inicializar el estado si es None
    if state is None:
        state = {
            'history': [],
            'input_ids_history': None,
            'user_name': default_user_name,
            'chatbot_name': default_chatbot_name,
            'story_context': default_story_context,
            'response_word_limit': default_response_word_limit,
            'seed': default_seed,
            'personality': default_personality,
            'user_language': 'es',
            'generation_params': {
                'temperature': 0.7,
                'top_p': 0.9,
                'repetition_penalty': 1.1
            },
            'user_photo': None,
            'chatbot_photo': None
        }

    if reset:
        # Reiniciar el historial de conversación y estado
        state['history'] = []
        state['input_ids_history'] = None
        # Actualizar configuraciones si se proporcionan
        state['user_name'] = new_user_name or default_user_name
        state['chatbot_name'] = new_chatbot_name or default_chatbot_name
        state['story_context'] = new_story_context or default_story_context
        state['response_word_limit'] = int(new_word_limit) if new_word_limit else default_response_word_limit
        state['seed'] = int(seed_input) if seed_input else default_seed
        state['personality'] = personality_choice or default_personality
        state['user_language'] = 'es'
        state['generation_params']['temperature'] = temperature
        state['generation_params']['top_p'] = top_p
        state['generation_params']['repetition_penalty'] = repetition_penalty

        # Procesar las imágenes cargadas y convertirlas a base64
        if user_photo:
            user_avatar = process_image(user_photo)
            state['user_photo'] = user_avatar
        else:
            state['user_photo'] = None

        if chatbot_photo:
            chatbot_avatar = process_image(chatbot_photo)
            state['chatbot_photo'] = chatbot_avatar
        else:
            state['chatbot_photo'] = None

        # No añadir mensaje inicial
        return [], state
    else:
        # Verificar si las configuraciones han cambiado
        settings_changed = False
        if new_user_name and new_user_name != state['user_name']:
            state['user_name'] = new_user_name
            settings_changed = True
        if new_chatbot_name and new_chatbot_name != state['chatbot_name']:
            state['chatbot_name'] = new_chatbot_name
            settings_changed = True
        if new_story_context and new_story_context != state['story_context']:
            state['story_context'] = new_story_context
            settings_changed = True
        if new_word_limit and int(new_word_limit) != state['response_word_limit']:
            state['response_word_limit'] = int(new_word_limit)
            settings_changed = True
        if seed_input and int(seed_input) != state.get('seed', default_seed):
            state['seed'] = int(seed_input)
            settings_changed = True
        if personality_choice and personality_choice != state.get('personality', default_personality):
            state['personality'] = personality_choice
            settings_changed = True
        if temperature != state['generation_params']['temperature']:
            state['generation_params']['temperature'] = temperature
            settings_changed = True
        if top_p != state['generation_params']['top_p']:
            state['generation_params']['top_p'] = top_p
            settings_changed = True
        if repetition_penalty != state['generation_params']['repetition_penalty']:
            state['generation_params']['repetition_penalty'] = repetition_penalty
            settings_changed = True
        if user_photo != state.get('user_photo', None):
            if user_photo:
                user_avatar = process_image(user_photo)
                state['user_photo'] = user_avatar
            else:
                state['user_photo'] = None
            settings_changed = True
        if chatbot_photo != state.get('chatbot_photo', None):
            if chatbot_photo:
                chatbot_avatar = process_image(chatbot_photo)
                state['chatbot_photo'] = chatbot_avatar
            else:
                state['chatbot_photo'] = None
            settings_changed = True

        if settings_changed and not user_input:
            # Si las configuraciones han cambiado pero no hay entrada del usuario, no generar respuesta
            return state['history'], state

        if user_input.strip() == "":
            # Si no hay entrada del usuario, devolver el historial actual
            return state['history'], state

        # Añadir el mensaje del usuario al historial para mostrar en la interfaz
        state['history'].append(('user', user_input))

        # Generar respuesta de forma asíncrona
        loop = asyncio.get_event_loop()
        state = await loop.run_in_executor(None, generate_text, user_input, state)

        # Preparar el historial para mostrar roles y avatares
        styled_history = []
        for message in state['history']:
            if message[0] == 'user':
                styled_history.append({
                    'role': 'user',
                    'content': message[1],
                    'name': state['user_name'],
                    'avatar': state.get('user_photo', None)
                })
            else:
                styled_history.append({
                    'role': 'assistant',
                    'content': message[1],
                    'name': state['chatbot_name'],
                    'avatar': state.get('chatbot_photo', None)
                })

        return styled_history, state

def main():
    """
    Configura el modelo y lanza la interfaz interactiva con Gradio.
    """
    print("Cargando el modelo...")
    load_model()

    print("Iniciando la interfaz...")
    with gr.Blocks() as interface:
        state = gr.State()
        gr.Markdown("## Chatbot by ViajaTech")

        # Historial de conversación en la parte superior
        chatbot_display = gr.Chatbot(label="Historial de la Conversación", height=600, type="messages")

        # Sección de entrada y configuraciones debajo del chat
        with gr.Column():
            with gr.Row():
                user_input = gr.Textbox(label="Entrada del Usuario", placeholder="Escribe aquí tu mensaje", lines=2)
                submit_btn = gr.Button("Enviar")
            with gr.Row():
                new_user_name = gr.Textbox(label="Nombre del Usuario (opcional)", placeholder="Tu nombre")
                new_chatbot_name = gr.Textbox(label="Nombre del Chatbot (opcional)", placeholder="Nombre del chatbot")
            new_story_context = gr.Textbox(label="Contexto o Historia Inicial (opcional)", placeholder="Escribe aquí un contexto o historia", lines=4)
            new_word_limit = gr.Slider(label="Palabras Máximas en Respuesta", minimum=10, maximum=200, step=10, value=50)
            reset = gr.Checkbox(label="Reiniciar Conversación", value=False)
            temperature = gr.Slider(label="Temperature", minimum=0.1, maximum=1.0, step=0.1, value=0.7)
            top_p = gr.Slider(label="Top-p", minimum=0.1, maximum=1.0, step=0.1, value=0.9)
            repetition_penalty = gr.Slider(label="Repetition Penalty", minimum=1.0, maximum=2.0, step=0.1, value=1.1)
            seed_input = gr.Textbox(label="Semilla (Seed) para Aleatoriedad (opcional)", placeholder="Ingresa un número entero")
            personality_choice = gr.Dropdown(label="Perfil de Personalidad", choices=list(personality_profiles.keys()), value=default_personality)
            user_photo = gr.Image(label="Foto del Usuario (opcional)", type="pil")
            chatbot_photo = gr.Image(label="Foto del Chatbot (opcional)", type="pil")

        inputs = [
            user_input,
            new_user_name,
            new_chatbot_name,
            new_story_context,
            new_word_limit,
            reset,
            temperature,
            top_p,
            repetition_penalty,
            seed_input,
            personality_choice,
            user_photo,
            chatbot_photo,
            state
        ]

        outputs = [
            chatbot_display,
            state
        ]

        submit_btn.click(
            chat_interface,
            inputs=inputs,
            outputs=outputs
        )

    interface.launch(share=True)

if __name__ == "__main__":
    main()
