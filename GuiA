#pip install torch torchvision torchaudio transformers accelerate bitsandbytes gradio asyncio langdetect
import asyncio
import os
import random
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed
import gradio as gr
import torch
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException

# Tu token de Hugging Face (asegúrate de reemplazar "tu_token_aquí" con tu token real)
HF_TOKEN = os.getenv("HF_TOKEN", "tu_token_aquí")

# Configuración global para el modelo y tokenizador
model = None
tokenizer = None

# Configuración personalizada por defecto
default_user_name = "David"
default_chatbot_name = "Isabella"
default_response_word_limit = 50
default_story_context = ""
default_seed = None  # Semilla por defecto (None para aleatoriedad completa)
default_personality = "Amigable"

# Perfiles de Personalidad Predefinidos
personality_profiles = {
    'Amigable': "Es amigable y utiliza un lenguaje coloquial.",
    'Profesional': "Es muy formal y profesional en sus respuestas.",
    'Humorístico': "Siempre responde con humor apropiado.",
    'Empático': "Es muy empática y muestra comprensión en sus respuestas.",
    'Creativo': "Ofrece respuestas creativas y originales.",
    # Puedes agregar más perfiles según desees
}

def load_model():
    """
    Carga el modelo LLaMA 3.2-3B con optimización de memoria.
    """
    global model, tokenizer
    llama_model_name = "meta-llama/Llama-3.2-3B"
    print("Cargando el tokenizador...")

    # Cargar el tokenizador con autenticación
    tokenizer = AutoTokenizer.from_pretrained(
        llama_model_name,
        use_fast=False,
        token=HF_TOKEN
    )

    print("Cargando el modelo...")
    # Configuración de BitsAndBytes para optimización en 8-bit
    bnb_config = BitsAndBytesConfig(
        load_in_8bit=True
    )

    # Cargar el modelo con autenticación
    model = AutoModelForCausalLM.from_pretrained(
        llama_model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
        token=HF_TOKEN
    )

    print("Modelo cargado exitosamente.")

def set_random_seed(seed):
    """
    Establece la semilla para los generadores de números aleatorios.
    """
    if seed is not None:
        set_seed(seed)
        random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

def detect_language(text):
    """
    Detecta el idioma del texto proporcionado.
    """
    try:
        return detect(text)
    except LangDetectException:
        return 'es'  # Asume español por defecto

def infer_gender(name):
    """
    Infiera el género basado en el nombre (simplificado).
    """
    if name.endswith('a') or name.endswith('A'):
        return 'ella'
    else:
        return 'él'

def generate_text(user_input, state):
    """
    Genera texto basado en el historial de conversación y la entrada del usuario.
    """
    if model is None or tokenizer is None:
        raise ValueError("El modelo no está cargado. Llama a load_model() primero.")

    # Establecer la semilla si se proporciona
    seed = state.get('seed', default_seed)
    set_random_seed(seed)

    # Desempaquetar variables de estado
    input_ids_history = state.get('input_ids_history', None)
    story_context = state['story_context']
    response_word_limit = state['response_word_limit']
    user_name = state['user_name'] or default_user_name
    chatbot_name = state['chatbot_name'] or default_chatbot_name
    generation_params = state['generation_params']
    personality = state.get('personality', default_personality)
    user_language = state.get('user_language', 'es')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Detectar el idioma del usuario
    user_language = detect_language(user_input)
    state['user_language'] = user_language

    # Inferir el género del chatbot
    chatbot_gender = infer_gender(chatbot_name)

    # Construir el prompt inicial si es la primera interacción
    if input_ids_history is None:
        # Prompt inicial con etiquetas claras
        prompt = f"""El siguiente es un diálogo entre {user_name} y su asistente virtual {chatbot_name}.

Contexto:
{chatbot_name} es {chatbot_gender} asistente virtual que {personality_profiles.get(personality, '')}
{story_context}

Conversación:
{user_name}: {user_input}
{chatbot_name}:"""

        # Tokenizar el prompt inicial
        input_ids_history = tokenizer.encode(prompt, return_tensors='pt').to(device)
    else:
        # Añadir el mensaje del usuario al historial
        new_message = f"\n{user_name}: {user_input}\n{chatbot_name}:"
        new_user_input_ids = tokenizer.encode(new_message, return_tensors='pt').to(device)
        input_ids_history = torch.cat([input_ids_history, new_user_input_ids], dim=-1)

    # Verificar si el input_ids_history excede la longitud máxima
    max_length = 2048
    if input_ids_history.size(1) > max_length:
        # Recortar el historial eliminando los tokens más antiguos
        input_ids_history = input_ids_history[:, -max_length:]

    # Generar la respuesta del modelo
    outputs = model.generate(
        input_ids_history,
        max_new_tokens=200,
        temperature=generation_params['temperature'],
        top_p=generation_params['top_p'],
        repetition_penalty=generation_params['repetition_penalty'],
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        eos_token_id=tokenizer.encode(f"\n{user_name}:", add_special_tokens=False)[0],
    )

    # Extraer la respuesta generada
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Extraer solo la última respuesta del chatbot
    response = generated_text.split(f"{chatbot_name}:")[-1].split(f"{user_name}:")[0].strip()

    # Limitar la respuesta a la cantidad máxima de palabras
    if response_word_limit > 0:
        response = " ".join(response.split()[:response_word_limit])

    # Actualizar el estado
    state['input_ids_history'] = outputs

    # Actualizar el historial de mensajes para mostrar en la interfaz
    state['history'].append(('assistant', response))

    return state

async def chat_interface(user_input, new_user_name, new_chatbot_name, new_story_context, new_word_limit, reset, temperature, top_p, repetition_penalty, seed_input, personality_choice, user_photo, chatbot_photo, state):
    """
    Interfaz para interactuar con el modelo desde Gradio.
    """
    # Inicializar el estado si es None
    if state is None:
        state = {
            'history': [],
            'input_ids_history': None,
            'user_name': default_user_name,
            'chatbot_name': default_chatbot_name,
            'story_context': default_story_context,
            'response_word_limit': default_response_word_limit,
            'seed': default_seed,
            'personality': default_personality,
            'user_language': 'es',
            'generation_params': {
                'temperature': 0.7,
                'top_p': 0.9,
                'repetition_penalty': 1.1
            },
            'user_photo': None,
            'chatbot_photo': None
        }

    if reset:
        # Reiniciar el historial de conversación y estado
        state['history'] = []
        state['input_ids_history'] = None
        # Actualizar configuraciones si se proporcionan
        state['user_name'] = new_user_name or default_user_name
        state['chatbot_name'] = new_chatbot_name or default_chatbot_name
        state['story_context'] = new_story_context or default_story_context
        state['response_word_limit'] = int(new_word_limit) if new_word_limit else default_response_word_limit
        state['seed'] = int(seed_input) if seed_input else default_seed
        state['personality'] = personality_choice or default_personality
        state['user_language'] = 'es'
        state['generation_params']['temperature'] = temperature
        state['generation_params']['top_p'] = top_p
        state['generation_params']['repetition_penalty'] = repetition_penalty
        state['user_photo'] = user_photo
        state['chatbot_photo'] = chatbot_photo
        # No añadir mensaje inicial
        return [], state
    else:
        # Verificar si las configuraciones han cambiado
        settings_changed = False
        if new_user_name and new_user_name != state['user_name']:
            state['user_name'] = new_user_name
            settings_changed = True
        if new_chatbot_name and new_chatbot_name != state['chatbot_name']:
            state['chatbot_name'] = new_chatbot_name
            settings_changed = True
        if new_story_context and new_story_context != state['story_context']:
            state['story_context'] = new_story_context
            settings_changed = True
        if new_word_limit and int(new_word_limit) != state['response_word_limit']:
            state['response_word_limit'] = int(new_word_limit)
            settings_changed = True
        if seed_input and int(seed_input) != state.get('seed', default_seed):
            state['seed'] = int(seed_input)
            settings_changed = True
        if personality_choice and personality_choice != state.get('personality', default_personality):
            state['personality'] = personality_choice
            settings_changed = True
        if temperature != state['generation_params']['temperature']:
            state['generation_params']['temperature'] = temperature
            settings_changed = True
        if top_p != state['generation_params']['top_p']:
            state['generation_params']['top_p'] = top_p
            settings_changed = True
        if repetition_penalty != state['generation_params']['repetition_penalty']:
            state['generation_params']['repetition_penalty'] = repetition_penalty
            settings_changed = True
        if user_photo != state.get('user_photo', None):
            state['user_photo'] = user_photo
            settings_changed = True
        if chatbot_photo != state.get('chatbot_photo', None):
            state['chatbot_photo'] = chatbot_photo
            settings_changed = True

        if settings_changed and not user_input:
            # Si las configuraciones han cambiado pero no hay entrada del usuario, no generar respuesta
            return [], state

        if user_input.strip() == "":
            # Si no hay entrada del usuario, devolver el historial actual
            return [], state

        # Añadir el mensaje del usuario al historial para mostrar en la interfaz
        state['history'].append(('user', user_input))

        # Generar respuesta de forma asíncrona
        loop = asyncio.get_event_loop()
        state = await loop.run_in_executor(None, generate_text, user_input, state)

        # Preparar el historial para mostrar roles y avatares con colores
        styled_history = []
        for message in state['history']:
            if message[0] == 'user':
                styled_history.append({
                    'role': 'user',
                    'content': f"<span style='color: blue;'>{message[1]}</span>",
                    'name': state['user_name'],
                    'avatar_url': state.get('user_photo', None)
                })
            else:
                styled_history.append({
                    'role': 'assistant',
                    'content': f"<span style='color: pink;'>{message[1]}</span>",
                    'name': state['chatbot_name'],
                    'avatar_url': state.get('chatbot_photo', None)
                })

        return styled_history, state

def main():
    """
    Configura el modelo y lanza la interfaz interactiva con Gradio.
    """
    print("Cargando el modelo...")
    load_model()

    print("Iniciando la interfaz...")
    interface = gr.Interface(
        fn=chat_interface,
        inputs=[
            gr.Textbox(label="Entrada del Usuario", placeholder="Escribe aquí tu mensaje", lines=2),
            gr.Textbox(label="Nombre del Usuario (opcional)", placeholder="Tu nombre"),
            gr.Textbox(label="Nombre del Chatbot (opcional)", placeholder="Nombre del chatbot"),
            gr.Textbox(label="Contexto o Historia Inicial (opcional)", placeholder="Escribe aquí un contexto o historia", lines=4),
            gr.Slider(label="Palabras Máximas en Respuesta", minimum=10, maximum=200, step=10, value=50),
            gr.Checkbox(label="Reiniciar Conversación", value=False),
            gr.Slider(label="Temperature", minimum=0.1, maximum=1.0, step=0.1, value=0.7),
            gr.Slider(label="Top-p", minimum=0.1, maximum=1.0, step=0.1, value=0.9),
            gr.Slider(label="Repetition Penalty", minimum=1.0, maximum=2.0, step=0.1, value=1.1),
            gr.Textbox(label="Semilla (Seed) para Aleatoriedad (opcional)", placeholder="Ingresa un número entero"),
            gr.Dropdown(label="Perfil de Personalidad", choices=list(personality_profiles.keys()), value=default_personality),
            gr.Image(label="Foto del Usuario (opcional)", type="filepath"),
            gr.Image(label="Foto del Chatbot (opcional)", type="filepath"),
            gr.State()  # Estado
        ],
        outputs=[
            gr.Chatbot(label="Historial de la Conversación", height=800, type="messages"),
            gr.State()  # Estado
        ],
        title="Chatbot by ViajaTech",
        description=(
            "Interactúa con el modelo LLaMA 3.2-3B localmente. Personaliza nombres, contexto inicial y límite de palabras. "
            "Ajusta los parámetros de generación, establece una semilla para influir en la personalidad del chatbot y elige un perfil de personalidad. "
            "Puedes agregar fotos para el usuario y el chatbot. "
            "Presiona Enter para enviar tu mensaje y continuar la conversación."
        ),
        live=False,
        # Ajustar el ancho de la interfaz
        theme="default",
        css="""
        #component-0 {
            width: 800px !important;
        }
        """
    )

    interface.launch(share=True)

if __name__ == "__main__":
    main()

